{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo0Edbo8O0HtVP5fV2YAnz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naqirraza99/CS-351L---AI-Lab1_2022574/blob/main/2022574_CS_351LAB_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question1: Designing a Neural Network for Student Performance Prediction**"
      ],
      "metadata": {
        "id": "_lUJ2sA2YHg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine we're trying to predict if a student will pass a test. We'd look at their attendance, past quiz scores, assignment grades, etc. A prediction model is like a sophisticated method: it combines these ingredients (data) in a special way (algorithm) to predict the outcome. The goal is to understand what factors lead to success so that teachers and schools can help each student reach their potential.\n",
        "\n",
        "This code will output the accuracy of the model for each specified number of neurons."
      ],
      "metadata": {
        "id": "llyJdrqVdzYz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdDfihsVLl23"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the Dataset**"
      ],
      "metadata": {
        "id": "s9wrPbSTY61i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset of **student_performance**\n"
      ],
      "metadata": {
        "id": "BrfSlihjeYDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('student_performance.csv')\n"
      ],
      "metadata": {
        "id": "h5EFIv2dN83B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Preprocess the Dataset**"
      ],
      "metadata": {
        "id": "wmKek9NnZlbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle the missing values from the dataset, we use mean strategy. The missing values in the column is replaced by the mean of the column."
      ],
      "metadata": {
        "id": "krkQSRwUek04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values (impute with mean)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data[['Attendance(%)', 'Assignment Score', 'Quiz Score','Final Exam Score']] = imputer.fit_transform(data[['Attendance(%)', 'Assignment Score', 'Quiz Score','Final Exam Score']])"
      ],
      "metadata": {
        "id": "pTUKaC05ZcQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X = data[['Attendance(%)', 'Assignment Score', 'Quiz Score','Final Exam Score']].values\n",
        "y = data['Pass/Fail'].values\n"
      ],
      "metadata": {
        "id": "DYs50guKZyGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Splits the data into training and test sets using an 80/20 split, and a fixed random state for reproducibility."
      ],
      "metadata": {
        "id": "rhlSK_2JhVn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "L4FBLxSLZy32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Fr-vHc3naE73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Build and Train Neural Network**"
      ],
      "metadata": {
        "id": "r7ZF3-feaKWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines a function called build_and_train_model that takes the number of neurons for the hidden layer, the training data, and the testing data as input."
      ],
      "metadata": {
        "id": "_-tPFy_ufPIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_and_train_model(neurons, X_train, y_train, X_test, y_test):\n",
        "    # Design the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1])) # Hidden layer with 'relu' activation\n",
        "    model.add(Dense(1, activation='sigmoid')) # Output layer with 'sigmoid' for binary classification\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.2, verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "b947RHvbaKtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Experiment with hidden layer neurons**"
      ],
      "metadata": {
        "id": "8wxgHWD9aZrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines a list of different numbers of neurons to test in the hidden layer."
      ],
      "metadata": {
        "id": "9HW5rvDlfWOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neuron_numbers = [5, 10, 20] #neuron numbers\n",
        "for neurons in neuron_numbers:\n",
        "    accuracy = build_and_train_model(neurons, X_train, y_train, X_test, y_test)\n",
        "    print(f\"Test Accuracy with {neurons} neurons: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv2yKc3TaVwE",
        "outputId": "32caa15e-14a5-4c38-ac44-a030fbeed50d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy with 5 neurons: 0.5\n",
            "Test Accuracy with 10 neurons: 1.0\n",
            "Test Accuracy with 20 neurons: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question2: Hyperparameter Tuning for Weather Prediction**"
      ],
      "metadata": {
        "id": "7gl31LhETi9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code loads data about temperature, humidity, wind speed, and weather conditions from a file. It cleans the data by converting weather conditions into numbers, makes sure all the values are on similar scales, and separates the data into \"features\" (temperature, humidity, etc.) and what we want to predict (whether it rained). It sets up a neural network (a type of machine learning model) that will analyze the relationships between weather conditions and rainfall. It trains the model on part of the data and then tests its ability to predict rain on the remaining part of the data, using different settings (learning rate and epochs). By running tests with different learning rates and epochs, it finds the settings that give the best accuracy.\n",
        "\n",
        "This code will output the test accuracy for each combination of learning rate and number of epochs, which helps you to identify what parameters provide the best predictive performance."
      ],
      "metadata": {
        "id": "_2V4qrRCfmxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "S9sdBNJybwnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the Dataset**"
      ],
      "metadata": {
        "id": "MPeN7K_Zb29z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset of **weather_data**"
      ],
      "metadata": {
        "id": "GZ8oFvuNgLMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('weather_data.csv')"
      ],
      "metadata": {
        "id": "vTGPwhgMb8Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Preprocess the Dataset**"
      ],
      "metadata": {
        "id": "OtAEfWM3b-OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This column contains 1 for rainy days and 0 for non-rainy days."
      ],
      "metadata": {
        "id": "CqOkiLvNgwxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X = data.drop('Rain (0/1)', axis=1)\n",
        "y = data['Rain (0/1)'].values"
      ],
      "metadata": {
        "id": "n-NFNVCWcfXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines a list of numerical and catagorical columns\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VUlk7OqRg1rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical and categorical columns\n",
        "numerical_cols = ['Temperature', 'Humidity (%)', 'Wind Speed (km/h)']\n",
        "categorical_cols = ['Weather Condition']"
      ],
      "metadata": {
        "id": "r-gSDnHscmas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a ColumnTransformer object to apply different preprocessing steps to different columns."
      ],
      "metadata": {
        "id": "tZtwwg_thKLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "Oi1Smy1bcoJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Splits the data into training and test sets using an 80/20 split, and a fixed random state for reproducibility."
      ],
      "metadata": {
        "id": "4kwnSqX5hbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5T9nUqcbcqN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)"
      ],
      "metadata": {
        "id": "WFAbFAOgcsdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Build and Train the Neural Network**"
      ],
      "metadata": {
        "id": "Qw-7d96mc0JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_and_train_model(lr, epochs, X_train, y_train, X_test, y_test):\n",
        "    # Design the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(10, activation='relu', input_dim=X_train.shape[1]))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model with specified learning rate\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=10, validation_split=0.2, verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "qJqC7CHhcuyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "_hVjyOxWdCMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Defines a list of learning rates and epochs to test and iterates through the learning rates and epochs lists. Prints the accuracy of the model after training with the current parameters."
      ],
      "metadata": {
        "id": "7suEpxvhhqYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Hyperparameter Tuning ---\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "epochs_list = [10, 50, 100]\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for epochs in epochs_list:\n",
        "        accuracy = build_and_train_model(lr, epochs, X_train, y_train, X_test, y_test)\n",
        "        print(f\"Test Accuracy with learning rate {lr} and {epochs} epochs: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "308tSBGodAOw",
        "outputId": "bab32a2a-5348-4ff1-e713-1d3097b2f069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy with learning rate 0.001 and 10 epochs: 0.5\n",
            "Test Accuracy with learning rate 0.001 and 50 epochs: 0.5\n",
            "Test Accuracy with learning rate 0.001 and 100 epochs: 0.5\n",
            "Test Accuracy with learning rate 0.01 and 10 epochs: 0.5\n",
            "Test Accuracy with learning rate 0.01 and 50 epochs: 0.5\n",
            "Test Accuracy with learning rate 0.01 and 100 epochs: 0.5\n",
            "Test Accuracy with learning rate 0.1 and 10 epochs: 0.5\n",
            "Test Accuracy with learning rate 0.1 and 50 epochs: 0.5\n",
            "Test Accuracy with learning rate 0.1 and 100 epochs: 0.5\n"
          ]
        }
      ]
    }
  ]
}